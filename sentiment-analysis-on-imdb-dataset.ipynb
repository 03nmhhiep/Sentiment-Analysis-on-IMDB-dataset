{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-03T12:54:17.781186Z","iopub.status.busy":"2023-07-03T12:54:17.780496Z","iopub.status.idle":"2023-07-03T12:54:17.800236Z","shell.execute_reply":"2023-07-03T12:54:17.798271Z","shell.execute_reply.started":"2023-07-03T12:54:17.780904Z"},"trusted":true},"outputs":[],"source":["import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T12:54:17.804543Z","iopub.status.busy":"2023-07-03T12:54:17.803995Z","iopub.status.idle":"2023-07-03T12:54:17.810172Z","shell.execute_reply":"2023-07-03T12:54:17.808773Z","shell.execute_reply.started":"2023-07-03T12:54:17.804454Z"},"trusted":true},"outputs":[],"source":["# pytorch_pretained_bert already available in kaggle conda env.\n","# !pip install pytorch-nlp"]},{"cell_type":"markdown","metadata":{},"source":["### importing necessaries libraries..."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T12:54:17.813148Z","iopub.status.busy":"2023-07-03T12:54:17.812208Z","iopub.status.idle":"2023-07-03T12:54:27.021595Z","shell.execute_reply":"2023-07-03T12:54:27.020420Z","shell.execute_reply.started":"2023-07-03T12:54:17.813086Z"},"trusted":true},"outputs":[],"source":["import sys\n","import numpy as np\n","import random as rn\n","import pandas as pd\n","import torch\n","from pytorch_pretrained_bert import BertModel\n","from torch import nn\n","# from torchnlp.datasets import imdb_dataset      # --> We are using our own uploaded dataset.\n","from pytorch_pretrained_bert import BertTokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim import Adam\n","from torch.nn.utils import clip_grad_norm_\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["### Initializing seed values to stabilize the outcomes."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T12:54:27.023853Z","iopub.status.busy":"2023-07-03T12:54:27.023419Z","iopub.status.idle":"2023-07-03T12:54:27.034958Z","shell.execute_reply":"2023-07-03T12:54:27.033760Z","shell.execute_reply.started":"2023-07-03T12:54:27.023772Z"},"trusted":true},"outputs":[],"source":["rn.seed(321)\n","np.random.seed(321)\n","torch.manual_seed(321)\n","torch.cuda.manual_seed(321)"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare the data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T12:54:27.039452Z","iopub.status.busy":"2023-07-03T12:54:27.039063Z","iopub.status.idle":"2023-07-03T12:54:28.439912Z","shell.execute_reply":"2023-07-03T12:54:28.438377Z","shell.execute_reply.started":"2023-07-03T12:54:27.039396Z"},"trusted":true},"outputs":[],"source":["path = '../input/imdb-50k-movie-reviews-test-your-bert/'\n","\n","train_data = pd.read_csv(path + 'train.csv')\n","test_data = pd.read_csv(path + 'test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T12:54:28.444280Z","iopub.status.busy":"2023-07-03T12:54:28.443881Z","iopub.status.idle":"2023-07-03T12:54:28.493355Z","shell.execute_reply":"2023-07-03T12:54:28.491871Z","shell.execute_reply.started":"2023-07-03T12:54:28.444211Z"},"trusted":true},"outputs":[],"source":["# experimenting here with a sample of dataset, to avoid memory overflow.\n","train_data = train_data[:2000]\n","test_data = test_data[:500]\n","\n","train_data = train_data.to_dict(orient='records')\n","test_data = test_data.to_dict(orient='records')\n","type(train_data)"]},{"cell_type":"markdown","metadata":{},"source":["### Mapping sentences with their Labels..."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T12:54:28.495829Z","iopub.status.busy":"2023-07-03T12:54:28.495391Z","iopub.status.idle":"2023-07-03T12:54:28.508283Z","shell.execute_reply":"2023-07-03T12:54:28.507078Z","shell.execute_reply.started":"2023-07-03T12:54:28.495752Z"},"trusted":true},"outputs":[],"source":["train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\n","test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n","\n","len(train_texts), len(train_labels), len(test_texts), len(test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T12:54:28.510150Z","iopub.status.busy":"2023-07-03T12:54:28.509839Z","iopub.status.idle":"2023-07-03T12:54:31.484802Z","shell.execute_reply":"2023-07-03T12:54:31.483432Z","shell.execute_reply.started":"2023-07-03T12:54:28.510104Z"},"trusted":true},"outputs":[],"source":["vocab = set()\n","\n","for x in train_texts:\n","    words = x.split()\n","    a = set(words)\n","    vocab = vocab.union(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T12:54:31.486878Z","iopub.status.busy":"2023-07-03T12:54:31.486441Z","iopub.status.idle":"2023-07-03T12:54:32.566109Z","shell.execute_reply":"2023-07-03T12:54:32.565173Z","shell.execute_reply.started":"2023-07-03T12:54:31.486804Z"},"trusted":true},"outputs":[],"source":["for x in test_texts:\n","    words = x.split()\n","    a = set(words)\n","    vocab = vocab.union(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T12:54:32.568368Z","iopub.status.busy":"2023-07-03T12:54:32.567949Z","iopub.status.idle":"2023-07-03T12:54:32.599986Z","shell.execute_reply":"2023-07-03T12:54:32.598646Z","shell.execute_reply.started":"2023-07-03T12:54:32.568290Z"},"trusted":true},"outputs":[],"source":["word_to_ix = {word: i for i, word in enumerate(vocab)}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T12:54:32.602182Z","iopub.status.busy":"2023-07-03T12:54:32.601887Z","iopub.status.idle":"2023-07-03T12:54:56.098131Z","shell.execute_reply":"2023-07-03T12:54:56.096829Z","shell.execute_reply.started":"2023-07-03T12:54:32.602131Z"},"trusted":true},"outputs":[],"source":["!python -m spacy download en_core_web_md"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T12:54:56.100783Z","iopub.status.busy":"2023-07-03T12:54:56.100327Z","iopub.status.idle":"2023-07-03T12:54:56.723899Z","shell.execute_reply":"2023-07-03T12:54:56.719424Z","shell.execute_reply.started":"2023-07-03T12:54:56.100713Z"},"trusted":true},"outputs":[],"source":["import en_core_web_md"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.725134Z","iopub.status.idle":"2023-07-03T12:54:56.725576Z"},"trusted":true},"outputs":[],"source":["# Load pre-trained GloVe model\n","nlp = en_core_web_md.load()\n","# nlp = spacy.load('en_core_web_md')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.727551Z","iopub.status.idle":"2023-07-03T12:54:56.728098Z"},"trusted":true},"outputs":[],"source":["# Get word embeddings\n","word_embeddings = {}\n","for word in vocab:\n","    word_embeddings[word] = nlp(word).vector"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.729211Z","iopub.status.idle":"2023-07-03T12:54:56.729755Z"},"trusted":true},"outputs":[],"source":["type(word_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.730862Z","iopub.status.idle":"2023-07-03T12:54:56.731296Z"},"trusted":true},"outputs":[],"source":["df = pd.DataFrame.from_dict(word_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.733021Z","iopub.status.idle":"2023-07-03T12:54:56.733562Z"},"trusted":true},"outputs":[],"source":["df.to_csv('/kaggle/working/embedding.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.734918Z","iopub.status.idle":"2023-07-03T12:54:56.735439Z"},"trusted":true},"outputs":[],"source":["train_embeddings = []\n","for x in train_texts:\n","    sentence_embed = []\n","    for word in x.split():\n","        sentence_embed.append(word_embeddings[word])\n","    train_embeddings.append(sentence_embed)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.736807Z","iopub.status.idle":"2023-07-03T12:54:56.737459Z"},"trusted":true},"outputs":[],"source":["test_embeddings = []\n","for x in test_texts:\n","    sentence_embed = []\n","    for word in x.split():\n","        sentence_embed.append(word_embeddings[word])\n","    test_embeddings.append(sentence_embed)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.739068Z","iopub.status.idle":"2023-07-03T12:54:56.739575Z"},"trusted":true},"outputs":[],"source":["train_embeddings = np.array(train_embeddings)\n","train_embeddings = pad_sequences(train_embeddings, maxlen=512, truncating=\"post\", padding=\"post\", dtype=float)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.741180Z","iopub.status.idle":"2023-07-03T12:54:56.741801Z"},"trusted":true},"outputs":[],"source":["test_embeddings = np.array(test_embeddings)\n","test_embeddings = pad_sequences(test_embeddings, maxlen=512, truncating=\"post\", padding=\"post\", dtype=float)"]},{"cell_type":"markdown","metadata":{},"source":["#### visualizing one of the sentences from train set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.743426Z","iopub.status.idle":"2023-07-03T12:54:56.743956Z"},"trusted":true},"outputs":[],"source":["train_texts[0]"]},{"cell_type":"markdown","metadata":{},"source":["## visualizing sentences lengths"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.745109Z","iopub.status.idle":"2023-07-03T12:54:56.745665Z"},"trusted":true},"outputs":[],"source":["sentences = [len(sent) for sent in train_texts]\n","\n","plt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})\n","plt.bar(range(1,2001), sentences, color = ['red'])\n","plt.gca().set(title='No. of characters in each sentence', xlabel='Number of sentence', ylabel='Number of Characters in each sentence');"]},{"cell_type":"markdown","metadata":{},"source":["#### We can see that most of the sentences are around 700 - 1000 characters long, which is pretty obvious. HOwever, few sentences are shorter and few even long as 6000 characters. So, this is a good, very versatile Review Dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.747295Z","iopub.status.idle":"2023-07-03T12:54:56.747752Z"},"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.748770Z","iopub.status.idle":"2023-07-03T12:54:56.749177Z"},"trusted":true},"outputs":[],"source":["tokenizer.tokenize('Hi my name is Atul')"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing Token embeddings..."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.750371Z","iopub.status.idle":"2023-07-03T12:54:56.750928Z"},"trusted":true},"outputs":[],"source":["train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n","test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n","\n","len(train_tokens), len(test_tokens)"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing Token Ids...\n","\n","\n","![token ids](https://jalammar.github.io/images/distilBERT/sst2-text-to-tokenized-ids-bert-example.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.752057Z","iopub.status.idle":"2023-07-03T12:54:56.752539Z"},"trusted":true},"outputs":[],"source":["train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","\n","train_tokens_ids.shape, test_tokens_ids.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.753602Z","iopub.status.idle":"2023-07-03T12:54:56.754129Z"},"trusted":true},"outputs":[],"source":["train_y = np.array(train_labels) == 'pos'\n","test_y = np.array(test_labels) == 'pos'\n","train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)"]},{"cell_type":"markdown","metadata":{},"source":["### Now Masking few random IDs from each sentences to remove Biasness from model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.755218Z","iopub.status.idle":"2023-07-03T12:54:56.755737Z"},"trusted":true},"outputs":[],"source":["train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n","test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"]},{"cell_type":"markdown","metadata":{},"source":["# Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.756953Z","iopub.status.idle":"2023-07-03T12:54:56.757394Z"},"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.758424Z","iopub.status.idle":"2023-07-03T12:54:56.759109Z"},"trusted":true},"outputs":[],"source":["baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression()).fit(train_texts, train_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.760582Z","iopub.status.idle":"2023-07-03T12:54:56.761111Z"},"trusted":true},"outputs":[],"source":["baseline_predicted = baseline_model.predict(test_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.762255Z","iopub.status.idle":"2023-07-03T12:54:56.762678Z"},"trusted":true},"outputs":[],"source":["print(classification_report(test_labels, baseline_predicted))"]},{"cell_type":"markdown","metadata":{},"source":["#### Our baseline model is working just fine and yeilding a fair enough score. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.763956Z","iopub.status.idle":"2023-07-03T12:54:56.764541Z"},"trusted":true},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = 'cuda'\n","else:\n","    device = 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.765870Z","iopub.status.idle":"2023-07-03T12:54:56.766312Z"},"trusted":true},"outputs":[],"source":["class TransformerModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim1, hidden_dim2, num_layers, num_heads):\n","        super(TransformerModel, self).__init__()\n","    \n","        # Define the positional encoding layer\n","        self.pos_enc = PositionalEncoding(input_dim)\n","        self.dropout = nn.Dropout(p=0.3)\n","        \n","        # Define the Transformer encoder\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads)\n","        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers, norm=nn.LayerNorm(input_dim)).to(device)\n","        \n","        temporal_encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads)\n","        self.temporal_encoder = nn.TransformerEncoder(temporal_encoder_layer, num_layers=num_layers, norm=nn.LayerNorm(input_dim)).to(device)\n","\n","        self.layer_norm = nn.LayerNorm(input_dim).to(device)\n","        # Define the fully connected layers\n","        self.fc1 = nn.Linear(input_dim, hidden_dim1).to(device)\n","        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2).to(device)\n","        self.fc3 = nn.Linear(hidden_dim2, 2).to(device)\n","\n","    def forward(self, x):\n","        b = x.shape[0]\n","        \n","        # Apply Transformer encoder to input\n","        x = self.encoder(x)\n","\n","        # Apply positional encoding to input\n","        x = self.pos_enc(x)\n","\n","        # Apply Temporal Transformer encoder to the Encoded Feature of Spatial Transformer\n","        x = self.temporal_encoder(x)\n","        x = self.dropout(x)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = x.view(b, -1)\n","        return x\n","\n","#         # Flatten the output from the Transformer\n","#         x = x.flatten(start_dim=1)\n","#         # Dimension is now input_dim x 124 after being flattened\n","#         x = self.layer_norm(x)\n","\n","#         # Apply fully connected layers\n","#         x = self.fc1(x)\n","#         x = nn.functional.gelu(x)\n","#         x = self.fc2(x)\n","#         x = nn.functional.gelu(x)\n","#         x = self.fc3(x)\n","        \n","#         x = x.view(b, -1)\n","\n","#         return x\n","\n","# Define the positional encoding layer\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n","        \"\"\"\n","        x = x + self.pe[:x.size(0)].to(device)\n","        return self.dropout(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.767446Z","iopub.status.idle":"2023-07-03T12:54:56.768041Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","# class PolicyNetwork(nn.Module):\n","#     def __init__(self, input_size, hidden_size):\n","#         super(PolicyNetwork, self).__init__()\n","#         self.fc1 = nn.Linear(input_size, hidden_size).to(device)\n","#         self.fc2 = nn.Linear(hidden_size, 16).to(device)\n","#         self.output = nn.Linear(16, 1).to(device)\n","\n","#     def forward(self, state):\n","#         x = F.relu(self.fc1(state))\n","#         x = F.relu(self.fc2(x))\n","#         output = torch.sigmoid(self.output(x))\n","#         return output\n","    \n","class PolicyNetwork(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers = 2):\n","        super(PolicyNetwork, self).__init__()\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True).to(device)\n","        self.conv1d_1 = nn.Conv1d(hidden_size, 64, kernel_size=3, stride=1, padding=1).to(device)\n","        self.conv1d_2 = nn.Conv1d(64, 32, kernel_size=3, stride=1, padding=1).to(device)\n","        self.fc = nn.Linear(32, 1).to(device)\n","#         self.sigmoid = F.sigmoid().to(device)\n","\n","    def forward(self, x):\n","        lstm_output, _ = self.lstm(x)\n","        lstm_output = lstm_output.permute(0, 2, 1)  # Reshape for conv1d\n","        conv1d_output = self.conv1d_1(lstm_output)\n","        conv1d_output = self.conv1d_2(conv1d_output)\n","        conv1d_output = conv1d_output.permute(0, 2, 1)  # Reshape back\n","        fc_output = self.fc(conv1d_output)\n","        output = torch.sigmoid(fc_output)\n","        return output\n","\n","\n","# class ClassificationNetwork(nn.Module):\n","#     def __init__(self, input_size, hidden_size, output_size):\n","#         super(ClassificationNetwork, self).__init__()\n","#         self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True).to(device)\n","#         self.fc = nn.Linear(hidden_size, output_size).to(device)\n","\n","#     def forward(self, x):\n","#         b, _, _ = x.shape\n","#         _, (hidden, _) = self.lstm(x)\n","#         output = self.fc(hidden.squeeze(0))\n","#         return output.view(b, -1)\n","\n","class ClassificationNetwork(nn.Module):\n","    def __init__(self, hidden_size, num_heads, ff_dim, dropout):\n","        super(ClassificationNetwork, self).__init__()\n","        self.self_attention = nn.MultiheadAttention(hidden_size, num_heads).to(device)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(hidden_size, ff_dim),\n","            nn.ReLU(),\n","            nn.Linear(ff_dim, hidden_size)\n","        ).to(device)\n","        self.layer_norm1 = nn.LayerNorm(hidden_size).to(device)\n","        self.layer_norm2 = nn.LayerNorm(hidden_size).to(device)\n","        self.dropout = nn.Dropout(dropout).to(device)\n","\n","    def forward(self, x):\n","        # Self-attention\n","        attention_output, _ = self.self_attention(x, x, x)\n","        attention_output = self.dropout(attention_output)\n","        x1 = self.layer_norm1(x + attention_output)\n","\n","        # Feed-forward network\n","        feed_forward_output = self.feed_forward(x1)\n","        feed_forward_output = self.dropout(feed_forward_output)\n","        x2 = self.layer_norm2(x1 + feed_forward_output)\n","\n","        return x2\n","\n","\n","# Example usage\n","num_heads = 4  # Number of attention heads\n","ff_dim = 512  # Dimension of the feed-forward network\n","dropout = 0.1  # Dropout rate\n","\n","num_classes = 2  # Number of output classes (binary sentiment)\n","num_layers = 2  # Number of transformer layers\n","def calculate_delayed_reward(probabilities, labels):\n","    positive_reward = torch.log(probabilities) * labels\n","    negative_reward = torch.log(1 - probabilities) * (1 - labels)\n","    delayed_reward = positive_reward + negative_reward\n","    return delayed_reward\n","\n","\n","# Define the input sizes and hyperparameters\n","input_size = 300  # Size of BERT embeddings\n","hidden_size = 256  # Size of hidden units in LSTM\n","output_size = 2  # Number of classes for sentence sentiment (binary classification)\n","\n","# Instantiate the policy network and classification network\n","policy_net = PolicyNetwork(input_size, hidden_size)\n","# classification_net = ClassificationNetwork(input_size, hidden_size, output_size, num_layers, num_heads)\n","# classification_net = ClassificationNetwork(hidden_size, num_heads, ff_dim, dropout)\n","\n","# Initialize the model\n","classification_net = TransformerModel(input_dim = 300, hidden_dim1 = 256, hidden_dim2 = 64, num_layers = 4, num_heads = 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.769340Z","iopub.status.idle":"2023-07-03T12:54:56.769905Z"},"trusted":true},"outputs":[],"source":["X_train = torch.Tensor(train_embeddings).to(device)\n","y_train = torch.tensor(train_y, dtype=torch.long).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.771150Z","iopub.status.idle":"2023-07-03T12:54:56.771979Z"},"trusted":true},"outputs":[],"source":["policy_loss_fn = nn.BCELoss()\n","policy_optimizer = torch.optim.RMSprop(policy_net.parameters(), lr=0.2)\n","\n","classification_loss_fn = nn.CrossEntropyLoss()\n","classification_optimizer = torch.optim.RMSprop(classification_net.parameters(), lr=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.773180Z","iopub.status.idle":"2023-07-03T12:54:56.773713Z"},"trusted":true},"outputs":[],"source":["batch_size = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.775013Z","iopub.status.idle":"2023-07-03T12:54:56.775563Z"},"trusted":true},"outputs":[],"source":["from torch.autograd import Variable"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.776800Z","iopub.status.idle":"2023-07-03T12:54:56.777359Z"},"trusted":true},"outputs":[],"source":["num_epochs = 1\n","for epoch in range(num_epochs):\n","    \n","    for i in range(0, X_train.shape[0], batch_size):\n","        # Get a batch of data\n","        x_batch = X_train[i:i+batch_size]\n","        labels = y_train[i:i+batch_size]\n","    \n","        # Zero the gradients for both networks\n","        policy_optimizer.zero_grad()\n","        classification_optimizer.zero_grad()\n","\n","        # Forward pass through the policy network\n","        probabilities = policy_net(x_batch)\n","        # Forward pass through the classification network\n","        selected_words = x_batch * probabilities\n","        prediction = classification_net(selected_words)\n","\n","        # Compute the classification loss and backpropagate\n","        classification_loss = classification_loss_fn(prediction, labels)\n","        classification_loss.backward()\n","        classification_optimizer.step()\n","        \n","        probabilities = probabilities.detach()\n","        \n","        # Calculate the delayed reward\n","        delayed_reward = calculate_delayed_reward(probabilities, labels)\n","        print(delayed_reward)\n","        \n","        # Compute the policy loss and backpropagate\n","        policy_loss = torch.sum(-delayed_reward)\n","        policy_loss = Variable(policy_loss, requires_grad = True)\n","        policy_loss.backward()\n","        policy_optimizer.step()\n","\n","\n","    # Print the loss for monitoring\n","    print(f\"Epoch {epoch + 1}: Policy Loss: {policy_loss.item()}, \"\n","          f\"Classification Loss: {classification_loss.item()}\")\n","\n","# After training, you can use the networks for inference."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.778820Z","iopub.status.idle":"2023-07-03T12:54:56.779357Z"},"trusted":true},"outputs":[],"source":["X_test = torch.Tensor(test_embeddings).to(device)\n","y_test = torch.tensor(test_y, dtype=torch.long).cpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.780805Z","iopub.status.idle":"2023-07-03T12:54:56.781341Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.782805Z","iopub.status.idle":"2023-07-03T12:54:56.783457Z"},"trusted":true},"outputs":[],"source":["with torch.no_grad():\n","    probabilities = policy_net(X_test)\n","    # Forward pass through the classification network\n","    selected_words = X_test * probabilities\n","    prediction = classification_net(selected_words).cpu()\n","    prediction = np.argmax(prediction, axis = 1)\n","\n","    # Compute the classification loss and backpropagate\n","    acc = accuracy_score(prediction, y_test)\n","print(acc)"]},{"cell_type":"markdown","metadata":{},"source":["# BERT Model\n","\n","\n","### Bidirectional Encoder Representations from Transformers. Each word here has a meaning to it and we will encounter that one by one in this article. For now, the key takeaway from this line is â€“ **BERT is based on the Transformer architecture**."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.784984Z","iopub.status.idle":"2023-07-03T12:54:56.785503Z"},"trusted":true},"outputs":[],"source":["class BertBinaryClassifier(nn.Module):\n","    def __init__(self, dropout=0.1):\n","        super(BertBinaryClassifier, self).__init__()\n","\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = nn.Linear(768, 1)\n","        self.sigmoid = nn.Sigmoid()\n","    \n","    def forward(self, tokens, masks=None):\n","        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n","        dropout_output = self.dropout(pooled_output)\n","        linear_output = self.linear(dropout_output)\n","        proba = self.sigmoid(linear_output)\n","        return proba"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.786963Z","iopub.status.idle":"2023-07-03T12:54:56.787444Z"},"trusted":true},"outputs":[],"source":["# ensuring that the model runs on GPU, not on CPU\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.788679Z","iopub.status.idle":"2023-07-03T12:54:56.789220Z"},"trusted":true},"outputs":[],"source":["str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.790348Z","iopub.status.idle":"2023-07-03T12:54:56.790934Z"},"trusted":true},"outputs":[],"source":["bert_clf = BertBinaryClassifier()\n","bert_clf = bert_clf.cuda()     # running BERT on CUDA_GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.792569Z","iopub.status.idle":"2023-07-03T12:54:56.793152Z"},"trusted":true},"outputs":[],"source":["str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.794686Z","iopub.status.idle":"2023-07-03T12:54:56.795234Z"},"trusted":true},"outputs":[],"source":["x = torch.tensor(train_tokens_ids[:3]).to(device)\n","y, pooled = bert_clf.bert(x, output_all_encoded_layers=False)\n","x.shape, y.shape, pooled.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.796611Z","iopub.status.idle":"2023-07-03T12:54:56.797207Z"},"trusted":true},"outputs":[],"source":["y = bert_clf(x)\n","y.cpu().detach().numpy()        # kinda Garbage Collector to free up used and cache space"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.798391Z","iopub.status.idle":"2023-07-03T12:54:56.798994Z"},"trusted":true},"outputs":[],"source":["# Cross- checking CUDA GPU Memory to ensure GPU memory is not overflowing.\n","str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.800978Z","iopub.status.idle":"2023-07-03T12:54:56.801603Z"},"trusted":true},"outputs":[],"source":["y, x, pooled = None, None, None\n","torch.cuda.empty_cache()     # Clearing Cache space for fresh Model run\n","str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"]},{"cell_type":"markdown","metadata":{},"source":["# Fine Tune BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.802848Z","iopub.status.idle":"2023-07-03T12:54:56.803398Z"},"trusted":true},"outputs":[],"source":["# Setting hyper-parameters\n","\n","BATCH_SIZE = 4\n","EPOCHS = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.805039Z","iopub.status.idle":"2023-07-03T12:54:56.805663Z"},"trusted":true},"outputs":[],"source":["train_tokens_tensor = torch.tensor(train_tokens_ids)\n","train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n","\n","test_tokens_tensor = torch.tensor(test_tokens_ids)\n","test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n","\n","train_masks_tensor = torch.tensor(train_masks)\n","test_masks_tensor = torch.tensor(test_masks)\n","\n","str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.807311Z","iopub.status.idle":"2023-07-03T12:54:56.807933Z"},"trusted":true},"outputs":[],"source":["train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.809177Z","iopub.status.idle":"2023-07-03T12:54:56.809661Z"},"trusted":true},"outputs":[],"source":["param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n","optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.810871Z","iopub.status.idle":"2023-07-03T12:54:56.811470Z"},"trusted":true},"outputs":[],"source":["optimizer = Adam(bert_clf.parameters(), lr=3e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.812899Z","iopub.status.idle":"2023-07-03T12:54:56.813617Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()   # Clearing Cache space for a fresh Model run"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.814836Z","iopub.status.idle":"2023-07-03T12:54:56.815403Z"},"trusted":true},"outputs":[],"source":["for epoch_num in range(EPOCHS):\n","    bert_clf.train()\n","    train_loss = 0\n","    for step_num, batch_data in enumerate(train_dataloader):\n","        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n","        print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n","        logits = bert_clf(token_ids, masks)\n","        \n","        loss_func = nn.BCELoss()\n","\n","        batch_loss = loss_func(logits, labels)\n","        train_loss += batch_loss.item()\n","        \n","        \n","        bert_clf.zero_grad()\n","        batch_loss.backward()\n","        \n","\n","        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        \n","        clear_output(wait=True)\n","        print('Epoch: ', epoch_num + 1)\n","        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.816557Z","iopub.status.idle":"2023-07-03T12:54:56.817137Z"},"trusted":true},"outputs":[],"source":["bert_clf.eval()\n","bert_predicted = []\n","all_logits = []\n","with torch.no_grad():\n","    for step_num, batch_data in enumerate(test_dataloader):\n","\n","        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n","\n","        logits = bert_clf(token_ids, masks)\n","        loss_func = nn.BCELoss()\n","        loss = loss_func(logits, labels)\n","        numpy_logits = logits.cpu().detach().numpy()\n","        \n","        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n","        all_logits += list(numpy_logits[:, 0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.818504Z","iopub.status.idle":"2023-07-03T12:54:56.819224Z"},"trusted":true},"outputs":[],"source":["np.mean(bert_predicted)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:56.820365Z","iopub.status.idle":"2023-07-03T12:54:56.820938Z"},"trusted":true},"outputs":[],"source":["print(classification_report(test_y, bert_predicted))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
